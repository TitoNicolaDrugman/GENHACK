{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCRIPT: build_ndvi_features.py\n",
    "# PURPOSE: Extracts and computes local, grid, and delta NDVI + land-cover\n",
    "#          fractions (water/urban/suburban/forest) for each station.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from pyproj import Transformer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 1: CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "DATA_FOLDER = r\"C:\\Users\\drugm\\Documents\\GENHACK\\GENHACK\\data\"\n",
    "\n",
    "NDVI_FOLDER = os.path.join(DATA_FOLDER, \"sentinel2_ndvi\")\n",
    "ECA_FOLDER = os.path.join(DATA_FOLDER, \"ECA_blend_tx\")\n",
    "STATIONS_FILE = os.path.join(ECA_FOLDER, \"stations.txt\")\n",
    "\n",
    "TARGET_COUNTRIES = ['ES', 'IT', 'FR', 'PT']\n",
    "\n",
    "WATER_MAX     = 0.10  # NDVI < 0.10\n",
    "URBAN_MAX     = 0.33  # 0.10 <= NDVI < 0.33\n",
    "SUBURBAN_MAX  = 0.60  # 0.33 <= NDVI < 0.60\n",
    "# NDVI >= 0.60 -> forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe747f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 2: LOAD STATION METADATA\n",
    "# =============================================================================\n",
    "\n",
    "def dms_to_decimal(dms_str):\n",
    "    \"\"\"Converts a Degrees:Minutes:Seconds string to decimal degrees.\"\"\"\n",
    "    if not isinstance(dms_str, str):\n",
    "        return None\n",
    "    original_dms_str = dms_str\n",
    "    dms_str = dms_str.strip().lstrip('+-')\n",
    "    parts = dms_str.split(':')\n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    val = float(parts[0]) + float(parts[1]) / 60 + float(parts[2]) / 3600\n",
    "    return -val if '-' in original_dms_str else val\n",
    "\n",
    "print(\"--- Loading Station Metadata ---\\n\")\n",
    "\n",
    "if not os.path.exists(STATIONS_FILE):\n",
    "    raise FileNotFoundError(f\"Error: Station file not found at {STATIONS_FILE}\")\n",
    "else:\n",
    "    df_meta = pd.read_csv(STATIONS_FILE, skiprows=17, skipinitialspace=True)\n",
    "    df_meta.columns = [c.strip() for c in df_meta.columns]\n",
    "    df_meta['CN'] = df_meta['CN'].str.strip()\n",
    "\n",
    "    df_meta = df_meta[df_meta['CN'].isin(TARGET_COUNTRIES)].copy()\n",
    "\n",
    "    df_meta['lat_dec'] = df_meta['LAT'].apply(dms_to_decimal)\n",
    "    df_meta['lon_dec'] = df_meta['LON'].apply(dms_to_decimal)\n",
    "    df_meta.dropna(subset=['lat_dec', 'lon_dec'], inplace=True)\n",
    "\n",
    "    station_locs = df_meta.set_index('STAID')[['lat_dec', 'lon_dec', 'HGHT']].to_dict('index')\n",
    "\n",
    "    print(f\"Found and processed {len(station_locs)} stations across {TARGET_COUNTRIES}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75661396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 3: SOPHISTICATED NDVI EXTRACTION (Grid vs. Point)\n",
    "# =============================================================================\n",
    "ERA5_GRID_SIZE =0.1\n",
    "HALF_GRID = ERA5_GRID_SIZE / 2.0\n",
    "\n",
    "station_mapping = []\n",
    "for staid, loc in station_locs.items():\n",
    "    era5_lat_center = round(loc['lat_dec'] / ERA5_GRID_SIZE) * ERA5_GRID_SIZE\n",
    "    era5_lon_center = round(loc['lon_dec'] / ERA5_GRID_SIZE) * ERA5_GRID_SIZE\n",
    "\n",
    "    station_mapping.append({\n",
    "        'STAID': staid,\n",
    "        'st_lat': loc['lat_dec'],\n",
    "        'st_lon': loc['lon_dec'],\n",
    "        'box_min_lat': era5_lat_center - HALF_GRID,\n",
    "        'box_max_lat': era5_lat_center + HALF_GRID,\n",
    "        'box_min_lon': era5_lon_center - HALF_GRID,\n",
    "        'box_max_lon': era5_lon_center + HALF_GRID\n",
    "    })\n",
    "df_geom = pd.DataFrame(station_mapping)\n",
    "\n",
    "ndvi_stats = {\n",
    "    staid: {\n",
    "        'local': [],\n",
    "        'grid': [],\n",
    "        'water_frac': [],\n",
    "        'urban_frac': [],\n",
    "        'suburban_frac': [],\n",
    "        'forest_frac': []\n",
    "    }\n",
    "    for staid in station_locs.keys()\n",
    "}\n",
    "\n",
    "ndvi_files=glob.glob(os.path.join(NDVI_FOLDER, \"*.tif\"))\n",
    "\n",
    "for tiff_path in tqdm(ndvi_files, desc=\"Processing Satellite Maps\"):\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', os.path.basename(tiff_path))\n",
    "    if not match:\n",
    "        continue\n",
    "    timestamp = pd.to_datetime(match.group(1))\n",
    "\n",
    "    if timestamp.month not in [6, 7, 8, 9]:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(tiff_path) as src:\n",
    "            transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n",
    "\n",
    "            for _, row in df_geom.iterrows():\n",
    "                staid = row['STAID']\n",
    "\n",
    "                try:\n",
    "                    x_point, y_point = transformer.transform(row['st_lon'], row['st_lat'])\n",
    "                    gen = src.sample([(x_point, y_point)])\n",
    "                    raw_val = next(gen)[0]\n",
    "                    if raw_val != 255:\n",
    "                        ndvi_local = (raw_val / 254.0 * 2.0) - 1.0\n",
    "                        if -1 <= ndvi_local <= 1:\n",
    "                            ndvi_stats[staid]['local'].append(ndvi_local)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    lons = [row['box_min_lon'], row['box_max_lon']]\n",
    "                    lats = [row['box_min_lat'], row['box_max_lat']]\n",
    "                    xs, ys = transformer.transform(lons, lats)\n",
    "                    left, bottom, right, top = min(xs), min(ys), max(xs), max(ys)\n",
    "\n",
    "                    window = from_bounds(left, bottom, right, top, transform=src.transform)\n",
    "                    data = src.read(1, window=window, boundless=True, fill_value=255)\n",
    "\n",
    "                    valid_data = data[data != 255]\n",
    "                    if valid_data.size > 0:\n",
    "                        valid_float = (valid_data / 254.0 * 2.0) - 1.0\n",
    "                        valid_float = valid_float[(valid_float >= -1.0) & (valid_float <= 1.0)]\n",
    "                        if valid_float.size == 0:\n",
    "                            continue\n",
    "\n",
    "                        grid_mean = np.mean(valid_float)\n",
    "                        if -1 <= grid_mean <= 1:\n",
    "                            ndvi_stats[staid]['grid'].append(grid_mean)\n",
    "\n",
    "\n",
    "                        water_mask = (valid_float < WATER_MAX)\n",
    "                        water_frac = np.mean(water_mask)\n",
    "                        ndvi_stats[staid]['water_frac'].append(water_frac)\n",
    "\n",
    "                        land_pixels = valid_float[~water_mask]\n",
    "                        if land_pixels.size == 0:\n",
    "                            ndvi_stats[staid]['urban_frac'].append(0.0)\n",
    "                            ndvi_stats[staid]['suburban_frac'].append(0.0)\n",
    "                            ndvi_stats[staid]['forest_frac'].append(0.0)\n",
    "                        else:\n",
    "                            urban_mask_land    = (land_pixels < URBAN_MAX) \n",
    "                            suburban_mask_land = (land_pixels >= URBAN_MAX) & (land_pixels < SUBURBAN_MAX)\n",
    "                            forest_mask_land   = (land_pixels >= SUBURBAN_MAX)\n",
    "\n",
    "                            ndvi_stats[staid]['urban_frac'].append(np.mean(urban_mask_land))\n",
    "                            ndvi_stats[staid]['suburban_frac'].append(np.mean(suburban_mask_land))\n",
    "                            ndvi_stats[staid]['forest_frac'].append(np.mean(forest_mask_land))\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {os.path.basename(tiff_path)} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a1721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Station Metadata ---\n",
      "\n",
      "âœ… Found and processed 2210 stations across ['ES', 'IT', 'FR', 'PT'].\n",
      "\n",
      "--- ðŸŒ¿ Starting Grid vs. Point NDVI Extraction ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Satellite Maps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:41<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SUCCESS! Calculated Grid vs Local NDVI for 2104 stations.\n",
      "Data saved to 'ndvi_static_features.csv'.\n",
      "\n",
      "--- Data Preview ---\n",
      "   STAID  ndvi_local  ndvi_global  delta_ndvi  perc_water  perc_urban  \\\n",
      "0     31    0.281496     0.260962    0.020534    0.169654    0.554595   \n",
      "1     32    0.518701     0.466386    0.052315    0.005724    0.224091   \n",
      "2     33    0.310039     0.356091   -0.046052    0.047130    0.383959   \n",
      "3     34    0.392717     0.552955   -0.160238    0.014184    0.119336   \n",
      "4     36    0.303150     0.319731   -0.016582    0.045924    0.549538   \n",
      "\n",
      "   perc_suburban  perc_forest  \n",
      "0       0.412429     0.032976  \n",
      "1       0.531286     0.244623  \n",
      "2       0.552516     0.063524  \n",
      "3       0.437106     0.443558  \n",
      "4       0.405299     0.045163  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 4: AGGREGATE RESULTS AND SAVE\n",
    "# =============================================================================\n",
    "\n",
    "final_ndvi_data = []\n",
    "for staid, data in ndvi_stats.items():\n",
    "    mean_local = np.mean(data['local']) if data['local'] else np.nan\n",
    "    mean_grid  = np.mean(data['grid'])  if data['grid']  else np.nan\n",
    "\n",
    "    if np.isnan(mean_local) or np.isnan(mean_grid):\n",
    "        continue\n",
    "\n",
    "    water_mean = np.mean(data['water_frac'])    if data['water_frac']    else np.nan\n",
    "    urban_mean  = np.mean(data['urban_frac'])    if data['urban_frac']    else np.nan\n",
    "    suburban_mean = np.mean(data['suburban_frac']) if data['suburban_frac'] else np.nan\n",
    "    forest_mean= np.mean(data['forest_frac'])   if data['forest_frac']   else np.nan\n",
    "\n",
    "    final_ndvi_data.append({\n",
    "        'STAID': staid,\n",
    "        'ndvi_local':   mean_local,\n",
    "        'ndvi_global':  mean_grid,\n",
    "        'delta_ndvi':   mean_local - mean_grid,\n",
    "        'perc_water':   water_mean,\n",
    "        'perc_urban':   urban_mean,\n",
    "        'perc_suburban': suburban_mean,\n",
    "        'perc_forest':  forest_mean\n",
    "    })\n",
    "\n",
    "df_ndvi_static = pd.DataFrame(final_ndvi_data)\n",
    "\n",
    "output_filename = \"ndvi_static_features.csv\"\n",
    "df_ndvi_static.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n SUCCESS! Calculated Grid vs Local NDVI for {len(df_ndvi_static)} stations.\")\n",
    "print(f\"Data saved to '{output_filename}'.\")\n",
    "print(\"\\n--- Data Preview ---\")\n",
    "print(df_ndvi_static.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a8e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ§  Found cache file. Loading pre-processed ERA5 data from:\n",
      "C:\\Users\\drugm\\Documents\\GENHACK\\GENHACK\\data\\era5_preloaded_cache.parquet ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72191e547ba845fb8bd276072103b6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reconstructing ERA5 dictionary from cache:   0%|          | 0/2210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Successfully loaded cached ERA5 data for 2210 stations.\n",
      "\n",
      "--- ðŸ‚ Loading NDVI static features ---\n",
      "âœ… NDVI data loaded.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: DATA PRE-LOADING (WITH CACHING)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "DATA_FOLDER = r\"C:\\Users\\drugm\\Documents\\GENHACK\\GENHACK\\data\"\n",
    "ERA5_FOLDER = os.path.join(DATA_FOLDER, \"derived-era5-land-daily-statistics\")\n",
    "ECA_FOLDER = os.path.join(DATA_FOLDER, \"ECA_blend_tx\")\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2025-12-31\"\n",
    "\n",
    "ERA5_CACHE_PATH = os.path.join(DATA_FOLDER, \"era5_preloaded_cache.parquet\")\n",
    "\n",
    "if os.path.exists(ERA5_CACHE_PATH):\n",
    "    print(f\"- Found cache file. Loading pre-processed ERA5 data from:\\n{ERA5_CACHE_PATH} ---\")\n",
    "    \n",
    "    cached_df = pd.read_parquet(ERA5_CACHE_PATH)\n",
    "    era5_dfs = {}\n",
    "    \n",
    "    for staid in tqdm(cached_df['STAID'].unique(), desc=\"Reconstructing ERA5 dictionary from cache\"):\n",
    "        station_df = (\n",
    "            cached_df[cached_df['STAID'] == staid]\n",
    "            .drop(columns=['STAID'])\n",
    "            .set_index('time')\n",
    "        )\n",
    "        era5_dfs[staid] = station_df\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded cached ERA5 data for {len(era5_dfs)} stations.\")\n",
    "\n",
    "else:\n",
    "    print(\"--No cache file found. Starting 20-minute pre-load from source .nc files... ---\")\n",
    "    \n",
    "    era5_files=glob.glob(os.path.join(ERA5_FOLDER, \"*.nc\"))\n",
    "    ds = xr.open_mfdataset(era5_files, combine='by_coords', parallel=True)\n",
    "    if 'valid_time' in ds.coords:\n",
    "        ds = ds.rename({'valid_time': 'time'})\n",
    "    rename_map = {\n",
    "        k: v for k, v in {\n",
    "            'mx2t': 'era5_temp_max',\n",
    "            't2m' : 'era5_temp_max',\n",
    "            'tp'  : 'era5_precip',\n",
    "            'u10' : 'era5_u10',\n",
    "            'v10' : 'era5_v10'\n",
    "        }.items() if k in ds\n",
    "    }\n",
    "    ds = ds.rename(rename_map)\n",
    "\n",
    "    era5_dfs = {}\n",
    "    for staid in tqdm(station_locs.keys(), desc=\"Extracting ERA5 Time Series\"):\n",
    "        try:\n",
    "            loc = station_locs[staid]\n",
    "            ds_point = ds.sel(\n",
    "                latitude=loc['lat_dec'],\n",
    "                longitude=loc['lon_dec'],\n",
    "                method='nearest'\n",
    "            ).load()\n",
    "            df_point = ds_point.to_dataframe()\n",
    "            df_point.reset_index(inplace=True)\n",
    "            df_point['time'] = pd.to_datetime(df_point['time']).dt.normalize()\n",
    "            df_point = df_point.drop_duplicates(subset=['time']).set_index('time')\n",
    "\n",
    "            if 'era5_temp_max' in df_point.columns:\n",
    "                df_point['era5_temp_max'] -= 273.15 \n",
    "            if 'era5_precip' in df_point.columns:\n",
    "                df_point['era5_precip'] *= 1000 \n",
    "\n",
    "            era5_dfs[staid] = df_point\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process ERA5 for station {staid}: {e}\")\n",
    "    \n",
    "    print(f\"\\n Successfully pre-loaded ERA5 data for {len(era5_dfs)} stations.\")\n",
    "    \n",
    "    print(\"\\n-Saving processed ERA5 data to cache for future runs... ---\")\n",
    "    if era5_dfs:\n",
    "        temp_list =[]\n",
    "        for staid, df in era5_dfs.items():\n",
    "            df_copy = df.reset_index().copy() \n",
    "            df_copy['STAID'] = staid\n",
    "            temp_list.append(df_copy)\n",
    "        \n",
    "        combined_df_to_cache = pd.concat(temp_list)\n",
    "        combined_df_to_cache.to_parquet(ERA5_CACHE_PATH)\n",
    "        print(f\"ERA5 data cached successfully to:\\n{ERA5_CACHE_PATH}\")\n",
    "    else:\n",
    "        print(\"No ERA5 data was loaded, so the cache file was not created.\")\n",
    "\n",
    "print(\"\\n--- Loading NDVI static features ---\")\n",
    "ndvi_static_file = \"ndvi_static_features.csv\"\n",
    "df_ndvi_static = pd.read_csv(ndvi_static_file).set_index('STAID')\n",
    "print(\"NDVI data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fe06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸš€ STEP 2: Starting final dataset construction ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5112cda5dd2349199bd484ab8522df4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging All Datasets:   0%|          | 0/2210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ”§ Merged data for 2110 stations. Ready for final processing. ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: MERGE ALL DATA SOURCES\n",
    "# =============================================================================\n",
    "print(\"\\n--- STEP 2: Starting final dataset construction ---\")\n",
    "training_list = []\n",
    "TARGET_STATIONS = list(station_locs.keys())\n",
    "\n",
    "for staid in tqdm(TARGET_STATIONS, desc=\"Merging All Datasets\"):\n",
    "    try:\n",
    "        if staid not in era5_dfs:\n",
    "            continue\n",
    "        df_e5 = era5_dfs[staid]\n",
    "\n",
    "        fpath = os.path.join(ECA_FOLDER, f\"TX_STAID{int(staid):06d}.txt\")\n",
    "        if not os.path.exists(fpath):\n",
    "            continue\n",
    "\n",
    "        df_st = pd.read_csv(fpath, skiprows=20, skipinitialspace=True)\n",
    "        df_st['time'] = pd.to_datetime(df_st['DATE'], format='%Y%m%d', errors='coerce')\n",
    "        df_st.dropna(subset=['time'], inplace=True)\n",
    "        df_st = df_st[df_st['Q_TX'] == 0]\n",
    "        df_st['TX'] = df_st['TX'] * 0.1  \n",
    "        df_st.set_index('time', inplace=True)\n",
    "        df_st = df_st.loc[START_DATE:END_DATE]\n",
    "        if df_st.empty:\n",
    "            continue\n",
    "\n",
    "        merged = pd.merge(df_st[['TX']], df_e5, left_index=True, right_index=True, how='inner')\n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        merged.reset_index(inplace=True) \n",
    "\n",
    "        merged['STAID'] = staid\n",
    "        merged = pd.merge(merged, df_ndvi_static, on='STAID', how='left')\n",
    "        training_list.append(merged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping station {staid} due to an unexpected error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n-- Merged data for {len(training_list)} stations. Ready for final processing. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df5b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finalizing dataset... ---\n",
      "\n",
      "âœ…âœ…âœ… SUCCESS! Your final dataset is ready! âœ…âœ…âœ…\n",
      "Saved 3034327 rows to 'euro_final_dataset_v2.csv'.\n",
      "\n",
      "--- Final Dataset Preview ---\n",
      "        time  STAID  latitude  longitude  elevation    TX  era5_temp_max  \\\n",
      "0 2020-01-01     32      47.1        2.4        161   7.1       8.108673   \n",
      "1 2020-01-02     32      47.1        2.4        161   9.2       9.584625   \n",
      "2 2020-01-03     32      47.1        2.4        161  10.9      10.435638   \n",
      "3 2020-01-04     32      47.1        2.4        161   8.5       8.464508   \n",
      "4 2020-01-05     32      47.1        2.4        161   9.0       7.048462   \n",
      "\n",
      "   delta_temp  ndvi_local  ndvi_global  ...  perc_urban  perc_suburban  \\\n",
      "0   -1.008673    0.518701     0.466386  ...    0.224091       0.531286   \n",
      "1   -0.384625    0.518701     0.466386  ...    0.224091       0.531286   \n",
      "2    0.464362    0.518701     0.466386  ...    0.224091       0.531286   \n",
      "3    0.035492    0.518701     0.466386  ...    0.224091       0.531286   \n",
      "4    1.951538    0.518701     0.466386  ...    0.224091       0.531286   \n",
      "\n",
      "   perc_forest  era5_u10  era5_v10  wind_speed  era5_precip  rain_7day_avg  \\\n",
      "0     0.244623 -0.132107  1.862945    1.867623     0.167214       0.167214   \n",
      "1     0.244623  1.078494  2.467729    2.693109     2.183162       1.319184   \n",
      "2     0.244623  2.976140  2.141345    3.666438     0.706658       1.054308   \n",
      "3     0.244623  1.091563 -1.706922    2.026103     0.097062       0.704229   \n",
      "4     0.244623 -1.169719 -1.185808    1.665648     0.102097       0.506860   \n",
      "\n",
      "    sin_day   cos_day  \n",
      "0  0.017166  0.999853  \n",
      "1  0.034328  0.999411  \n",
      "2  0.051479  0.998674  \n",
      "3  0.068615  0.997643  \n",
      "4  0.085731  0.996318  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: FINAL CALCULATIONS & SAVE (UPDATED FEATURES)\n",
    "# =============================================================================\n",
    "if training_list:\n",
    "    print(\"--- Finalizing dataset... ---\")\n",
    "\n",
    "    final_df = pd.concat(training_list, ignore_index=True)\n",
    "\n",
    "    final_df['time']=pd.to_datetime(final_df['time']).dt.normalize()\n",
    "    final_df.sort_values(by=['STAID', 'time'], inplace=True)\n",
    "\n",
    "    final_df['delta_temp'] = final_df['TX'] - final_df['era5_temp_max']\n",
    "    final_df['doy'] = final_df['time'].dt.dayofyear\n",
    "    final_df['sin_day'] = np.sin(2 * np.pi * final_df['doy'] / 366.0)\n",
    "    final_df['cos_day'] = np.cos(2 * np.pi * final_df['doy'] / 366.0)\n",
    "\n",
    "    final_df['wind_speed'] = np.sqrt(final_df['era5_u10']**2 + final_df['era5_v10']**2)\n",
    "\n",
    "    final_df['rain_7day_avg'] = final_df.groupby('STAID')['era5_precip'].transform(\n",
    "        lambda x: x.ewm(span=7).mean()\n",
    "    )\n",
    "    final_df['elevation'] = final_df['STAID'].map({k: v['HGHT'] for k, v in station_locs.items()})\n",
    "\n",
    "    final_columns = [\n",
    "        'time', 'STAID', 'latitude', 'longitude', 'elevation',\n",
    "        'TX', 'era5_temp_max', 'delta_temp',\n",
    "        'ndvi_local', 'ndvi_global', 'delta_ndvi',\n",
    "        'perc_water', 'perc_urban', 'perc_suburban', 'perc_forest',\n",
    "        'era5_u10', 'era5_v10', 'wind_speed',\n",
    "        'era5_precip', 'rain_7day_avg',\n",
    "        'sin_day', 'cos_day'\n",
    "    ]\n",
    "    final_columns_exist = [col for col in final_columns if col in final_df.columns]\n",
    "    final_df = final_df[final_columns_exist]\n",
    "    final_df.dropna(subset=['delta_temp', 'delta_ndvi'], inplace=True)\n",
    "\n",
    "    # Save\n",
    "    output_filename = \"euro_final_dataset_v2.csv\"\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\n SUCCESS! Your final dataset is ready! \")\n",
    "    print(f\"Saved {len(final_df)} rows to '{output_filename}'.\")\n",
    "    print(\"\\n--- Final Dataset Preview ---\")\n",
    "    print(final_df.head())\n",
    "else:\n",
    "    print(\"\\nCRITICAL ERROR: No data was merged. Check your file paths and station lists.\")\n",
    "\n",
    "for name in ['era5_dfs', 'training_list', 'final_df', 'ds']:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1a336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
